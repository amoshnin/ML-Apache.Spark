{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ee9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install pyspark library\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ed739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sparksession \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ee90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a sparksession object and providing appName \n",
    "spark=SparkSession.builder.appName(\"optimization\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05086a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create dataframe form External datasets\n",
    "AirlineDF = spark.read.option(\"header\", \"true\").csv(\"/Users/krishnapratap/Desktop/partation/data/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc4893",
   "metadata": {},
   "source": [
    "# use take() in place of collect() for reduce time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576cfd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total rows in dataframe\n",
    "%time AirlineDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate total columns in dataframe\n",
    "len(AirlineDF.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1825df7",
   "metadata": {},
   "source": [
    "#  Catalyst optimizer \n",
    "\n",
    "Catalyst Optimizer improves developer productivity and the performance of their written queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bbfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDF.select('OriginCityName') \\\n",
    ".filter(\"Distance >= 200\").filter(\"ArrDelayMinutes  >= 5\").filter(\"Year  >= 2005\").groupBy('OriginCityName') \\\n",
    ".count().withColumnRenamed(\"count\", \"Total Delay Flights\").orderBy(\"OriginCityName\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF.select('OriginCityName') \\\n",
    ".filter(\"Distance >= 200\").filter(\"ArrDelayMinutes  >= 5\").filter(\"Year  >= 2005\").groupBy('OriginCityName') \\\n",
    ".count().withColumnRenamed(\"count\", \"Total Delay Flights\").orderBy(\"OriginCityName\").explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea26bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDF.select('OriginCityName') \\\n",
    ".orderBy(\"OriginCityName\").withColumnRenamed(\"count\", \"Total Delay Flights\").filter((col(\"Year\") >= \"2005\") & (col(\"ArrDelayMinutes\") >= \"5\") & (col(\"Distance\") >= \"200\")).groupBy('OriginCityName') \\\n",
    ".count().orderBy(\"OriginCityName\").withColumnRenamed(\"count\", \"Total Delay Flights\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF.select('OriginCityName') \\\n",
    ".orderBy(\"OriginCityName\").withColumnRenamed(\"count\", \"Total Delay Flights\").filter((col(\"Year\") >= \"2005\") & (col(\"ArrDelayMinutes\") >= \"5\") & (col(\"Distance\") >= \"200\")).groupBy('OriginCityName') \\\n",
    ".count().orderBy(\"OriginCityName\").withColumnRenamed(\"count\", \"Total Delay Flights\").explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979aa898",
   "metadata": {},
   "source": [
    "explain(mode=\"simple\") shows physical plan.\n",
    "\n",
    "explain(mode=\"extended\") presents physical and logical plans.\n",
    "\n",
    "explain(mode=\"codegen\") shows the java code planned to be executed.\n",
    "\n",
    "explain(mode=\"cost\") presents the optimized logical plan and related statistics (if they exist).\n",
    "\n",
    "explain(mode=\"formatted\") shows a split output created by an optimized physical plan outline, and a section of every node detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac877f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acc9dd70",
   "metadata": {},
   "source": [
    "# When data is huge otherwise not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bc85a",
   "metadata": {},
   "source": [
    "# use coalesce() in place of repartition() to reduce the no. of partition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check how many partation in current AirlineDF dataframe\n",
    "AirlineDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF1 = AirlineDF.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF2 = AirlineDF.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab78022",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDF2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e432b03",
   "metadata": {},
   "source": [
    "# when we load data from other source like s3, database\n",
    "\n",
    "# when you dealing with heavy-weighted initialization on larger datasets.\n",
    "\n",
    "# select needed columns from entire dataset and write into new file then create new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# Create an RDD\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae91f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sc.textFile(\"/Users/krishnapratap/Desktop/partation/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92580c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ardd = df.map(lambda x: x.replace(', ',' ').split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineRdd=Ardd.map(lambda x: Row(Year=x[0],UniqueCarrier=x[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the sql context\n",
    "sqlContex = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDf1 = sqlContex.createDataFrame(AirlineRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDf1.write.option(\"header\", \"true\").csv(\"flight/airdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a431ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sparksession \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a sparksession object and providing appName \n",
    "spark=SparkSession.builder.appName(\"optimization\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create dataframe form External datasets\n",
    "AirlineDf2 = spark.read.option(\"header\", \"true\").csv(\"flight/airdata/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd95acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42679f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf2.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb986808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41613a36",
   "metadata": {},
   "source": [
    "# Apache Parquet is a columnar file format that provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON, supported by many data processing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AirlineDf2.write.parquet(\"parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "NewDfParquet = spark.read.parquet(\"parquet/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0699132",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time NewDfParquet.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284a3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d26cef6e",
   "metadata": {},
   "source": [
    "# Using cache() and persist() methods, Spark provides an optimization mechanism to store the intermediate computation of a Spark DataFrame so they can be reused in subsequent actions.\n",
    "\n",
    "# Note - When NOT to use Cache/Persist- When the size of the data is large and there are multiple dfs available for cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before rdd is persisted\n",
    "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf1.select(\"Year\").groupby(\"Year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56196bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b186934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist dataframe to memory and disk\n",
    "AirlineDf1.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c787df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf1.select(\"Year\").groupby(\"Year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22676d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpersist dataframe\n",
    "AirlineDf1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c95737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persist dataframe to memory \n",
    "AirlineDf1.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64455924",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf1.select(\"Year\").groupby(\"Year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af40447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpersist dataframe\n",
    "AirlineDf1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e06243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persist dataframe to disk\n",
    "AirlineDf1.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf1.select(\"Year\").groupby(\"Year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7485df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67927e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpersist dataframe\n",
    "AirlineDf1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd197a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ba871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a1032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache dataframe\n",
    "AirlineDf1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf1.select(\"Year\").groupby(\"Year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f03bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962bfc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unchache dataframe \n",
    "AirlineDf1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after rdd is uncached\n",
    "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3021112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec454f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3026150",
   "metadata": {},
   "source": [
    "# use reduceByKey() in place of Groupbykey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=sc.textFile(\"/Users/krishnapratap/Desktop/partation/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split words using flatMap\n",
    "rdd_word = rdd1.flatMap(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ef506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a paired-rdd\n",
    "rdd_pair = rdd_word.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5418bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurence per word using groupbykey()\n",
    "rdd_group = rdd_pair.groupByKey()\n",
    "rdd_group_count = rdd_group.map(lambda x:(x[0], len(x[1])))\n",
    "%time rdd_group_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurence per word using reducebykey()\n",
    "rdd_reduce = rdd_pair.reduceByKey(lambda x,y: x+y)\n",
    "%time rdd_reduce.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015ebb6",
   "metadata": {},
   "source": [
    "# use apache arrow to optimize query time \n",
    "\n",
    "# create dataframe in pyspark from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94f1bd",
   "metadata": {},
   "source": [
    "# here we are creating pandas dataframe from multiple csv file\n",
    "# so we need to import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ba2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1 = pd.read_csv('/Users/krishnapratap/Desktop/partation/data/On_Time_On_Time_Performance_2005_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a1c64",
   "metadata": {},
   "source": [
    "# how to enable arrow for fast pyspark dataframe creation from pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df2 = spark.createDataFrame(pdf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aeda9f",
   "metadata": {},
   "source": [
    "# how to disable arrow  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to cast all the columns in the pandas df to string type to overcome this datatype issue converting pandas df to spark df\n",
    "%time df1 = spark.createDataFrame(pdf1.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c049d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
